{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyankajuttu/Projects_and_Labs/blob/main/STP_AIML_Module_6__Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1**"
      ],
      "metadata": {
        "id": "OifQj4WLp-Ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Probabilistic ML models\n",
        "\n",
        "Topics:\n",
        "1. Convolutional Operation\n",
        "2. CNN and Using Learnt Representations\n",
        "3. CNN Visualization"
      ],
      "metadata": {
        "id": "V89R735GVNdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Convolutional Operations"
      ],
      "metadata": {
        "id": "0hAW8ptqVeyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central to Convolutional Neural Networks (CNN), a convolution operation is a linear operation which involves element-wise multiplication between a small filter (say, a matrix of integers) and filter-sized patch from the image. We move this filter across the image like a sliding window from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation. These filters can do simplest task like checking if there is a vertical line in the image or complicated task like detecting a human eye in the image.\n",
        "\n",
        "Let's look at the convolution formula:\n",
        "\n",
        "Convolution between image $f(x, y)$ and kernel $k(x, y)$ is\n",
        "$$f(x,y) * k(x,y) = \\sum \\limits _{i=0} ^{W-1} \\sum \\limits _{j=0} ^{H-1} f(i, j) k(x − i, y − j)$$\n",
        "\n",
        "where $W$ and $H$ are the the width and height of the image.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Chaim-Baskin/publication/318849314/figure/fig1/AS:614287726870532@1523469015098/Image-convolution-with-an-input-image-of-size-7-7-and-a-filter-kernel-of-size-3-3.png\" alt=\"Convolution\" width=650px height=280px/>\n",
        "\n",
        "\n",
        "Image reference: [Streaming Architecture for Large-Scale Quantized Neural Networks on an FPGA-Based Dataflow Platform](https://www.researchgate.net/publication/318849314_Streaming_Architecture_for_Large-Scale_Quantized_Neural_Networks_on_an_FPGA-Based_Dataflow_Platform/figures?lo=1)\n",
        "\n",
        "The code demonstrates the convolution operation of a 2D matrix (image) with various filters"
      ],
      "metadata": {
        "id": "hbpRXyTpVv7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igR7HFGhRRdm"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0],\n",
        "                   [1,1,0],\n",
        "                   [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0],\n",
        "                   [0,0,0],\n",
        "                   [1,1,1]])\n",
        "\n",
        "# print(image1*255)\n",
        "# Let's plot the images\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "ax.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "# plt.ylim(0, 3)\n",
        "ax.set_title('Image 1 with vertical edge')\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "ax.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "ax.set_title('Image 2 with horizontal edge')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WlakMr1Wlee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGZFq4O8-qql"
      },
      "source": [
        "Let's create a 3x3 vertical edge filter. We will 'convolve' this filter over the images to detect vertical edge. As the image is same size as of filter, this is simple element-wise multiplication and summing up the result into single value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ralw9w7R0x"
      },
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoWLcsu37jLq"
      },
      "source": [
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvCvYI9c-MX4"
      },
      "source": [
        "Non-zero output suggests that there is a vertical edge present in the first image and not present in the second image.\n",
        "Now, let's create a horizontal edge filter and apply it to both the above images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7h6HrJ8h5W"
      },
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BWlGBDVWEDO"
      },
      "source": [
        "As expected, the horizontal edge is detected in second image with this filter.\n",
        "\n",
        "Now, we will take a bigger image (5 x 5) and see how a convolution operation works by sliding a filter left to right and top to bottom to obtain an output map from image. Let's define a function ***apply_filter()*** for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VszqKiZHsOXB"
      },
      "source": [
        "def apply_filter(img, filter):\n",
        "  height, width = img.shape\n",
        "  filter_size = filter.shape\n",
        "\n",
        "  output = np.empty(0)\n",
        "\n",
        "  # Move the filter over entire image and store the result in output\n",
        "  for i in range(0, height - filter_size[1] + 1):\n",
        "    for j in range(0, width - filter_size[0] + 1):\n",
        "      # Matrix multiplication for a single patch of image and filter\n",
        "      output = np.append(output, np.sum(np.multiply(img[i:i+filter_size[0], j:j+filter_size[1]], filter)))\n",
        "\n",
        "  # Calculate the output shape of the resultant image\n",
        "  output_shape = (height - (filter_size[1]-1)), (width - (filter_size[0]-1))\n",
        "\n",
        "  # Return the reshaped image\n",
        "  return output.reshape(output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y03Dtn3F0sp"
      },
      "source": [
        "Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mJqbXQZF0sq"
      },
      "source": [
        "def plot_images(images, titles, tick_params=True):\n",
        "  n = len(images)\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(1,n,i+1)\n",
        "    if len(images[i].shape) == 2:\n",
        "      ax.imshow(images[i], cmap='gray',\n",
        "                extent=(0,images[i].shape[1], images[i].shape[0], 0))\n",
        "    else:\n",
        "      ax.imshow(images[i])\n",
        "    ax.set_title(titles[i])\n",
        "    if not tick_params:\n",
        "      plt.tick_params(axis='both', labelbottom=False, bottom=False,\n",
        "                labelleft=False, left=False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTLmPYG0Pom"
      },
      "source": [
        "# 2D image\n",
        "img = np.array([[20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0]])\n",
        "\n",
        "# Vertical edge filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "\n",
        "output = apply_filter(img, filter)\n",
        "print(output) # Note the shape of output image!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-tmlp1qHA9f"
      },
      "source": [
        "# Let's plot the above image with results\n",
        "images = []\n",
        "titles = []\n",
        "\n",
        "images.append(img)\n",
        "titles.append('Original Image')\n",
        "\n",
        "images.append(filter)\n",
        "titles.append('Filter')\n",
        "\n",
        "images.append(output)\n",
        "titles.append('Convolution Output')\n",
        "\n",
        "plot_images(images, titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSCGzaYVFze"
      },
      "source": [
        "As, you can see, horizontal edge is detected in the output.\n",
        "\n",
        "Now, we will see the effect of applying this filter on a grayscale image. Again, for this, we need to 'convolve' the filter over the entire image.\n",
        "We will use the same filter and function defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjOQM6tdF0sr"
      },
      "source": [
        "# Get the sample image\n",
        "!curl -L -o 'lotus.jpg' 'https://drive.google.com/uc?export=download&id=1gQSQlrUws22KLRUacXwvN1G8FtIyhfGt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04P_GfojIo4u"
      },
      "source": [
        "# Read the image with opencv, 0 stands for 'grayscale'\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "print('Original image size: ', image.shape)\n",
        "\n",
        "# Saving images for plots\n",
        "images = []\n",
        "titles = []\n",
        "\n",
        "images.append(image)\n",
        "titles.append('Original Image')\n",
        "\n",
        "# Vertical edge filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "images.append(filter)\n",
        "titles.append('Filter')\n",
        "\n",
        "# Apply this filter to image\n",
        "output = apply_filter(image, filter)\n",
        "\n",
        "print('Output image size: ', output.shape)\n",
        "\n",
        "images.append(output)\n",
        "titles.append('Convolution Output')\n",
        "\n",
        "# Let's plot the images\n",
        "plot_images(images, titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "1. Try padding in convolution on lotus.jpg and show results\n",
        "2. Try stride  in convolution on lotus.jpg and show results"
      ],
      "metadata": {
        "id": "bsQxPFm-YDmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CNN and Using Learnt Representations\n",
        "\n",
        "Now lets implement a CNN in pytorch and use the learnt representations for image classification of MNIST dataset."
      ],
      "metadata": {
        "id": "u1hVe_w-aOKe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewhnwh1P6aAk"
      },
      "source": [
        "<img src='https://miro.medium.com/max/1872/1*SGPGG7oeSvVlV5sOSQ2iZw.png' />\n",
        "\n",
        "Image reference: https://miro.medium.com/max/1872/1*SGPGG7oeSvVlV5sOSQ2iZw.png\n",
        "\n",
        "We will be implementing a CNN model which can predict the digit, given a grayscale image. The architecture of model is given in the above image.\n",
        "\n",
        "**We will do the following steps in order:**\n",
        "1.   Load and visualize MNIST training and test datasets using torchvision\n",
        "2.   Define the CNN model\n",
        "3.   Define a loss function and optimizer\n",
        "4.   Train the network on the training data\n",
        "5.   Evaluate the network on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWJa9NQvSdr8"
      },
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhYuybK767Sa"
      },
      "source": [
        "# Device configuration (whether to run on GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMFwB08ZvSN"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ0n6wa47uZu"
      },
      "source": [
        "#### Load MNIST data\n",
        "We will use the [MNIST dataset](https://pytorch.org/vision/stable/datasets.html#mnist) from torchvision Pytorch and setup the train and test dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr2uSnxn7nIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f722c4e-7d0c-4cc9-e953-7ee0f6aa05a3"
      },
      "source": [
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "\n",
        "# Images in torchvision datasets are PIL Images in range [0,1] so we need\n",
        "# 'ToTensor' transform to convert them into tensors\n",
        "train_data = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "test_data = torchvision.datasets.MNIST('./data', train=False, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test, shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 503kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.63MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.86MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HRj26o673We"
      },
      "source": [
        "#### Understand the dataset\n",
        "Let us now visualize the dataset in terms of number of samples, classes etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwP6Weyr7wZb",
        "outputId": "453e011d-5da8-44a7-dc14-84a5ff5e4e66"
      },
      "source": [
        "print('Training data shape : ', train_data.data.shape, train_data.targets.shape)\n",
        "print('Testing data shape : ', test_data.data.shape, test_data.targets.shape)\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "classes = np.unique(train_data.targets.numpy())\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape :  torch.Size([60000, 28, 28]) torch.Size([60000])\n",
            "Testing data shape :  torch.Size([10000, 28, 28]) torch.Size([10000])\n",
            "Total number of outputs :  10\n",
            "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ZzTl-078NJ"
      },
      "source": [
        "# Helper function to plot data\n",
        "def plot_data(images, labels, classes=None):\n",
        "  figure = plt.figure(figsize=(9, 4))\n",
        "  cols, rows = 5, 2\n",
        "  for i in range(1, cols * rows + 1):\n",
        "      sample_idx = torch.randint(len(images), size=(1,)).item()\n",
        "      img, label = images[sample_idx], labels[sample_idx]\n",
        "      figure.add_subplot(rows, cols, i)\n",
        "      if classes is not None:\n",
        "        label = classes[label]\n",
        "      plt.title('Label:' +str(label))\n",
        "      plt.axis(\"off\")\n",
        "      plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "  plt.show()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "W1bnP8EyqktD",
        "outputId": "3f25d255-eae9-41c1-957a-fdc302894b98"
      },
      "source": [
        "plot_data(train_data.data, train_data.targets.numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYYNFnPk8LWD"
      },
      "source": [
        "#### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-e1DrVT8GYt"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(64 * 7 * 7, 128)\n",
        "        # output layer 10 classes\n",
        "        self.out = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x) #activation\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.max_pool2(x)\n",
        "        # flatten the output for FC layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        output = self.out(x)\n",
        "        return output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqoGCFWd8RRG",
        "outputId": "1089d587-9a9f-4819-8410-a776e24c85a7"
      },
      "source": [
        "# Build the model object and put on the device\n",
        "model = CNN().to(device)\n",
        "print(model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux_2PGgT8diG"
      },
      "source": [
        "#### Define Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDAwCNb8VER"
      },
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fwSTPmI8lQP"
      },
      "source": [
        "#### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GieENlOa8heQ"
      },
      "source": [
        "# Basic SGD optimizer with 0.01 learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svJ3-UB187qa"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94F0wxMefKz"
      },
      "source": [
        "Helper function for training/testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDXNReyi8n5o"
      },
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Put data on devices\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFYI98OtNjCc"
      },
      "source": [
        "def test(model, test_loader):\n",
        "  # Eval mode\n",
        "  model.eval()\n",
        "  test_acc = 0\n",
        "  correct = 0\n",
        "  for i, (images, labels) in enumerate(test_loader):\n",
        "    # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "    with torch.no_grad():\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "      correct += (preds == labels).sum()\n",
        "\n",
        "  test_acc = correct / len(test_loader.dataset)\n",
        "  print('Test Accuracy: {:.4f}'.format(test_acc*100))\n",
        "\n",
        "  # Plot the images with predicted labels\n",
        "  plot_data(images.data.cpu().numpy(), preds.data.cpu().numpy(), test_loader.dataset.classes)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6pYjXgchc3E"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax9XMoe5H_ik",
        "outputId": "5365d4bc-28db-42df-ce18-d692f4edc9b1"
      },
      "source": [
        "num_epochs = 10  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, train_loader, loss_func, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.0115, Accuracy: 63.7583\n",
            "Epoch 2/10, Loss: 0.0029, Accuracy: 89.0600\n",
            "Epoch 3/10, Loss: 0.0022, Accuracy: 91.6233\n",
            "Epoch 4/10, Loss: 0.0018, Accuracy: 93.1517\n",
            "Epoch 5/10, Loss: 0.0015, Accuracy: 94.3717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ltVbRI4dr6"
      },
      "source": [
        "Plot training plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPJHC1_tIFAr"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYW-_wTfWvYl"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rReuhwjpXr5K"
      },
      "source": [
        "# Evaluate the model on testing data and plot predictions\n",
        "test(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMO_nSBmUFq"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfozvSgV_mt5"
      },
      "source": [
        "Q 1: What is the ratio of parameters in single 5 x 5 kernel and equivalent stacked 3 x 3 kernels? Consider number of channels in input and output channels as C."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Single 5×5 Convolution:\n",
        "\n",
        "* Parameters = C_in × C_out × 5 × 5 = 25 × C_in × C_out\n",
        "\n",
        "Stacked Two 3×3 Convolutions (Equivalent Receptive Field):\n",
        "\n",
        "* First 3×3 layer: C_in × C_mid × 3 × 3 = 9 × C_in × C_mid\n",
        "* Second 3×3 layer: C_mid × C_out × 3 × 3 = 9 × C_mid × C_out\n",
        "* If C_mid = C_in = C_out = C (commonly used):\n",
        "\n",
        "  *  Total = 9C² + 9C² = 18C²\n",
        "\n",
        "* For single 5×5: 25C²\n",
        "\n",
        "Ratio = 25C² : 18C² = 25:18 ≈ 1.39:1\n",
        "\n",
        "Conclusion: Two stacked 3×3 kernels use ~28% fewer parameters than a single 5×5 kernel while achieving the same receptive field!\n",
        "\n",
        "Additional Benefits of Stacked 3×3:\n",
        "\n",
        "* More non-linearities (two ReLUs instead of one)\n",
        "* Better feature extraction capability\n",
        "* Reduced computational cost\n",
        "* Better gradient flow"
      ],
      "metadata": {
        "id": "Q3lFbV1UHQFf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7LXbTpZnbT0"
      },
      "source": [
        "Q 2: How can you replace 7 x 7 convolution kernel using only 3 x 3 kernels? What would be ratio of parameters in this case? Consider number of channels in input and output channels as C."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Replacement Strategy:**\n",
        "A 7×7 convolution can be replaced by **THREE stacked 3×3 convolutions**\n",
        "\n",
        "**Receptive Field Calculation:**\n",
        "- 1st 3×3 conv: receptive field = 3×3\n",
        "- 2nd 3×3 conv: receptive field = 3 + (3-1) = 5×5\n",
        "- 3rd 3×3 conv: receptive field = 5 + (3-1) = **7×7** ✓\n",
        "\n",
        "**Parameter Comparison:**\n",
        "\n",
        "**Single 7×7 Convolution:**\n",
        "- Parameters = C_in × C_out × 7 × 7 = **49C²** (assuming C_in = C_out = C)\n",
        "\n",
        "**Three Stacked 3×3 Convolutions:**\n",
        "- 1st layer: C × C × 3 × 3 = 9C²\n",
        "- 2nd layer: C × C × 3 × 3 = 9C²\n",
        "- 3rd layer: C × C × 3 × 3 = 9C²\n",
        "- Total = **27C²**\n",
        "\n",
        "**Ratio = 49C² : 27C² = 49:27 ≈ 1.81:1**\n",
        "\n",
        "**Conclusion**: Three stacked 3×3 kernels use **~45% fewer parameters** than a single 7×7 kernel!\n",
        "\n",
        "**Why This Matters (VGGNet Innovation):**\n",
        "- VGGNet famously uses only 3×3 convolutions throughout\n",
        "- Achieves deeper networks with fewer parameters\n",
        "- More non-linearities = better feature learning\n",
        "- Computational efficiency increases"
      ],
      "metadata": {
        "id": "6wri1J3_Iyud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CNN Visualization"
      ],
      "metadata": {
        "id": "Tq_SyTxUbteW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga0Az1zLcPVz"
      },
      "source": [
        "Save the conv layers and their weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_ZpAdpvVts"
      },
      "source": [
        "model_weights = [] # we will save the conv layer weights in this list\n",
        "conv_layers = [] # we will save the conv layers in this list\n",
        "# get all the model children as list\n",
        "model_children = list(model.children())\n",
        "\n",
        "# counter to keep count of the conv layers\n",
        "counter = 0\n",
        "# append all the conv layers and their respective weights to the list\n",
        "for i in range(len(model_children)):\n",
        "    if type(model_children[i]) == nn.Conv2d:\n",
        "        counter += 1\n",
        "        model_weights.append(model_children[i].weight)\n",
        "        conv_layers.append(model_children[i])\n",
        "    elif type(model_children[i]) == nn.Sequential:\n",
        "        for j in range(len(model_children[i])):\n",
        "            for child in model_children[i][j].children():\n",
        "                if type(child) == nn.Conv2d:\n",
        "                    counter += 1\n",
        "                    model_weights.append(child.weight)\n",
        "                    conv_layers.append(child)\n",
        "print(f\"Total convolutional layers: {counter}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N34IYC8Fziq1"
      },
      "source": [
        "# take a look at the conv layers and the respective weights\n",
        "for weight, conv in zip(model_weights, conv_layers):\n",
        "    # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n",
        "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qhgNqhgcTpW"
      },
      "source": [
        "### Visualize the CONV layer filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCH5GBQpznZw"
      },
      "source": [
        "# Visualize the conv layer filters\n",
        "plt.figure(figsize=(20, 17))\n",
        "for i, filter in enumerate(model_weights[0]):\n",
        "    plt.subplot(8, 8, i+1) # (8, 8)\n",
        "    plt.imshow(filter[0, :, :].data.cpu().numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPNns-hcbWK"
      },
      "source": [
        "### Visualize filter outputs on an image\n",
        "Get an image from test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV_7rEx3zpuy"
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "for images, labels in dataiter:\n",
        "    img = images[1]\n",
        "    fig = plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img.reshape((28, 28)))\n",
        "    print(classes[labels[1].item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajfFCveAxpf-"
      },
      "source": [
        "Forward pass the image through saved conv layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR6LxW9t0ADw"
      },
      "source": [
        "results = [conv_layers[0](img.to(device))]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    # pass the result from the last layer to the next layer\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "# make a copy of the `results`\n",
        "outputs = results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkeMeBQdxwpP"
      },
      "source": [
        "Visualize features from each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDuxp0nq0BEX"
      },
      "source": [
        "for num_layer in range(len(outputs)):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    layer_viz = outputs[num_layer][:, :, :]\n",
        "    layer_viz = layer_viz.data\n",
        "    print('Layer output size:', layer_viz.size())\n",
        "    for i, filter in enumerate(layer_viz):\n",
        "        plt.subplot(8, 8, i + 1)\n",
        "        plt.imshow(filter.cpu().numpy(), cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "    print(f\"Layer {num_layer} feature maps...\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oniT6gixQZ7_"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5XatV34Qia0"
      },
      "source": [
        "Q: List a few practical applications of convolutional autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "**Convolutional Autoencoders - Practical Applications:**\n",
        "\n",
        "1. **Image Denoising**\n",
        "   - Remove noise from corrupted images\n",
        "   - Medical imaging: denoise X-rays, MRIs, CT scans\n",
        "   - Photography: enhance low-light images\n",
        "\n",
        "2. **Image Compression**\n",
        "   - Compress images to lower dimensions\n",
        "   - More efficient than traditional methods for specific domains\n",
        "   - Learned compression tailored to data distribution\n",
        "\n",
        "3. **Anomaly Detection**\n",
        "   - Manufacturing: detect defective products\n",
        "   - Medical: identify abnormal tissue/lesions\n",
        "   - Security: identify unusual patterns in surveillance\n",
        "\n",
        "4. **Image Inpainting**\n",
        "   - Fill in missing or corrupted parts of images\n",
        "   - Restore old photographs\n",
        "   - Remove unwanted objects from images\n",
        "\n",
        "5. **Super-Resolution**\n",
        "   - Upscale low-resolution images to high-resolution\n",
        "   - Enhance surveillance footage\n",
        "   - Medical imaging enhancement\n",
        "\n",
        "6. **Feature Extraction**\n",
        "   - Learn compact representations for downstream tasks\n",
        "   - Pre-training for classification/segmentation\n",
        "   - Transfer learning applications\n",
        "\n",
        "7. **Data Augmentation**\n",
        "   - Generate synthetic variations of training data\n",
        "   - Improve model robustness\n",
        "   - Handle class imbalance\n",
        "\n",
        "8. **Semantic Segmentation**\n",
        "   - Encoder-decoder architecture (U-Net)\n",
        "   - Medical image segmentation\n",
        "   - Autonomous driving (lane/object detection)\n",
        "\n",
        "9. **Image Colorization**\n",
        "   - Convert grayscale images to color\n",
        "   - Restore old black-and-white photos\n",
        "   - Artistic applications\n",
        "\n",
        "10. **Dimensionality Reduction for Visualization**\n",
        "    - Visualize high-dimensional image data\n",
        "    - Clustering and exploration\n",
        "    - t-SNE/UMAP preprocessing"
      ],
      "metadata": {
        "id": "UBmTs-O3I-Wc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqMzkKLBq18x"
      },
      "source": [
        "Q: What change do we need to make for the autoencoder to reduce into PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "To make an autoencoder reduce into PCA (Principal Component Analysis), we need:\n",
        "\n",
        "**Key Changes:**\n",
        "\n",
        "1. **Remove All Non-Linearities**\n",
        "   - Remove activation functions (ReLU, sigmoid, tanh)\n",
        "   - Use only linear transformations\n",
        "   - Network becomes: Encoder: X → W₁X, Decoder: W₂(W₁X)\n",
        "\n",
        "2. **Single Layer Architecture**\n",
        "   - Use only one encoding layer and one decoding layer\n",
        "   - No hidden layers or deep structure\n",
        "\n",
        "3. **Linear Loss Function**\n",
        "   - Use Mean Squared Error (MSE) as reconstruction loss\n",
        "   - L = ||X - X̂||² where X̂ is the reconstruction\n",
        "\n",
        "4. **No Regularization**\n",
        "   - Remove dropout, batch normalization\n",
        "   - No weight decay or L1/L2 penalties\n",
        "\n",
        "5. **Tied Weights (Optional but Important)**\n",
        "   - Set decoder weights as transpose of encoder: W₂ = W₁ᵀ\n",
        "   - Ensures orthonormality (like PCA)\n",
        "\n",
        "**Mathematical Equivalence:**\n",
        "\n",
        "When you train a linear autoencoder with:\n",
        "- Linear encoder: h = W₁ᵀX\n",
        "- Linear decoder: X̂ = W₂h = W₂W₁ᵀX\n",
        "- MSE loss: min ||X - W₂W₁ᵀX||²\n",
        "- Tied weights: W₂ = W₁\n",
        "\n",
        "**The solution converges to:**\n",
        "- W₁ = principal components (eigenvectors) of the covariance matrix XᵀX\n",
        "- h = principal component scores (projections onto eigenvectors)\n",
        "- This is exactly what PCA does!\n",
        "\n",
        "**Key Insight:**\n",
        "- PCA is a special case of autoencoders\n",
        "- Autoencoders generalize PCA by adding non-linearities\n",
        "- Linear autoencoder = PCA\n",
        "- Non-linear autoencoder = \"non-linear PCA\" or \"kernel PCA-like\""
      ],
      "metadata": {
        "id": "ov9n94aIJDjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 2**"
      ],
      "metadata": {
        "id": "aGuVyJV5w5Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following topics will be covered in this section:\n",
        "1. Effect of padding, kernel size and stride\n",
        "2. Pooling\n",
        "3. Transfer learning and fine-tuning"
      ],
      "metadata": {
        "id": "LmHzS_VvppeQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HjQRq6bVNtG"
      },
      "source": [
        "## 1. Effect of padding, kernel size and stride\n",
        "We will directly use convolution layer in the **Pytorch** framework. Refer [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for more information about additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RKg8qtjP6af"
      },
      "source": [
        "# Import pytorch packages\n",
        "import torch\n",
        "from torch.nn import Conv2d\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o 'lotus.jpg' 'https://drive.google.com/uc?export=download&id=1gQSQlrUws22KLRUacXwvN1G8FtIyhfGt'"
      ],
      "metadata": {
        "id": "daLFmh6HzGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBcuce2bF0su"
      },
      "source": [
        "##### Convolution in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmaXjhVCrdJ"
      },
      "source": [
        "We will define a helper function to create an square vertical edge filter of given size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX6K7F6-dHhf"
      },
      "source": [
        "def generate_filter(k=3):\n",
        "  kernel = np.ones((k, k))\n",
        "  mid_index = k // 2\n",
        "  kernel[:, mid_index].fill(0)\n",
        "  kernel[:, mid_index+1:] *= -1\n",
        "  return kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25gwS1GgMGY"
      },
      "source": [
        "We will create a helper function that takes one of the kernel elements, create a Convolution layer using pytorch and return the output image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a6_FwTsgZ49"
      },
      "source": [
        "def apply_conv(image, kernel_size, padding=0, stride=1):\n",
        "\n",
        "  #--------IMAGE PREPROCESSING-------\n",
        "  # Convert image to tensor from numpy\n",
        "  image = torch.from_numpy(image)\n",
        "  # Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "  # --------------KERNEL-------------\n",
        "  # Create a nxn kernel\n",
        "  kernel = generate_filter(kernel_size)\n",
        "\n",
        "  # Create a tensor from the numpy array\n",
        "  kernel = torch.from_numpy(kernel.astype(np.float32))\n",
        "\n",
        "  # Pytorch requires kernel of shape (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  kernel = kernel.view((1,1,kernel.shape[0], kernel.shape[1]))\n",
        "\n",
        "  # ---------CONVOLUTION LAYER--------\n",
        "  #1 input image channel, 1 output channels, nxn square convolution with padding on all 4 sides\n",
        "  conv = Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, padding=padding, stride=stride)\n",
        "\n",
        "  # Set the kernel weights in the convolution layer\n",
        "  conv.weight = torch.nn.Parameter(kernel)\n",
        "\n",
        "  # ---------APPLY CONVOLUTION--------\n",
        "  output = conv(input / 255.)  # Getting input from 0 to 1\n",
        "  output_img = output.data.numpy()  # Tensor to back in numpy\n",
        "  output_img = output_img.reshape((-1, output_img.shape[-1])) # Reshape to 2D image\n",
        "\n",
        "  return output_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6-_WvVRdp6l"
      },
      "source": [
        "##### Effect of Padding\n",
        "Change the padding value with the slider. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehT5KaDJeESD"
      },
      "source": [
        "#@title Effect of padding { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "# Note:After running this cell manually, it will auto-run if you\n",
        "# change the selected value.\n",
        "\n",
        "# Our original lotus image\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "\n",
        "# Apply 3x3 convolution to image with given padding 1 on all 4 sides\n",
        "padding = 2 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "n = apply_conv(image, 3, padding=padding)\n",
        "\n",
        "# Plot the results\n",
        "plt.imshow(n, cmap='gray')\n",
        "plt.title('Padding={}\\nShape: {}'.format(padding, str(n.shape)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyFlTUw7exTI"
      },
      "source": [
        "As you observed, the output shape changes with padding. More the padding, bigger will be the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVThLy3OfBi4"
      },
      "source": [
        "##### Effect of Kernel size\n",
        "Change the kernel size with the slider. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYoc2xwTlBj_"
      },
      "source": [
        "#@title Effect of Kernel size { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "# Our original lotus image\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "# Apply 3x3 convolution to image\n",
        "K = 19 #@param {type:\"slider\", min:3, max:21, step:2}\n",
        "n = apply_conv(image, K)\n",
        "\n",
        "# Plot result\n",
        "plt.imshow(n, cmap='gray')\n",
        "plt.title('K = {}\\nShape = {}'.format(K, str(n.shape)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oKNN4gBnzOc"
      },
      "source": [
        "Thus, we conclude that output image becomes blurry with increase in kernel size as summation occurs over larger neighbourhood. Smaller kernel size is used to capture details whereas larger kernel captures bigger elements in image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DJoZNQOmlT3"
      },
      "source": [
        "##### Effect of Stride\n",
        "Change the stride value with the slider. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zISU25SBj1O7"
      },
      "source": [
        "#@title Effect of Stride { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "# Our original lotus image\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "# Apply 3x3 convolution to image\n",
        "stride = 9 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "n = apply_conv(image, 3, stride=stride)\n",
        "\n",
        "# Plot result\n",
        "plt.imshow(n, cmap='gray')\n",
        "plt.title('Stride = {}\\nShape = {}'.format(stride, str(n.shape)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNSTgzuYoY29"
      },
      "source": [
        "As we can see, the output becomes pixelated as strides increase because we have fewer values in the output by skipping pixels in input. Also, hence size of output decreases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWI03dYYR2xf"
      },
      "source": [
        "## 2. Pooling\n",
        "Strides, actually downsample the image but a more robust and common approach is pooling. It may be useful when we do not require finer details but important structural elements. Here, we will see an example of max pooling and average pooling on a simple 2D image matrix. Refer [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [nn.AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) for the documentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVOCZ80mnrdf"
      },
      "source": [
        "Max Pooling & Average Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD1mlfc8JOjs"
      },
      "source": [
        "from torch.nn import MaxPool2d, AvgPool2d\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def plot_images(images, titles, tick_params=True):\n",
        "  n = len(images)\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(1,n,i+1)\n",
        "    if len(images[i].shape) == 2:\n",
        "      ax.imshow(images[i], cmap='gray',\n",
        "                extent=(0,images[i].shape[1], images[i].shape[0], 0))\n",
        "    else:\n",
        "      ax.imshow(images[i])\n",
        "    ax.set_title(titles[i])\n",
        "    if not tick_params:\n",
        "      plt.tick_params(axis='both', labelbottom=False, bottom=False,\n",
        "                labelleft=False, left=False)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# 2D image\n",
        "image = np.array([\n",
        "   \t\t[2, 0, 0, 1, 6, 0, 5, 3],\n",
        "\t\t[0, 6, 5, 9, 1, 5, 8, 0],\n",
        "\t\t[0, 6, 0, 1, 2, 3, 4, 4],\n",
        "\t\t[7, 0, 3, 1, 4, 2, 0, 1],\n",
        "\t\t[3, 7, 0, 5, 6, 0, 5, 0],\n",
        "\t\t[0, 4, 6, 1, 1, 0, 0, 1],\n",
        "\t\t[3, 5, 0, 2, 2, 0, 9, 0],\n",
        "\t\t[1, 0, 3, 1, 7, 0, 0, 0]])\n",
        "\n",
        "# Saving output for plots\n",
        "output = []\n",
        "titles = []\n",
        "\n",
        "output.append(image)\n",
        "titles.append('Image')\n",
        "\n",
        "image = torch.from_numpy(image.astype(np.float32))\n",
        "input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "#----------MAX POOLING LAYER--------\n",
        "pool_layer = MaxPool2d(kernel_size=4, stride=4)\n",
        "op = pool_layer(input)\n",
        "max_output_img = op.data.numpy()  # Tensor to back in numpy\n",
        "max_output_img = max_output_img.reshape((-1, max_output_img.shape[-1]))\n",
        "print('Max Pooling:\\n', max_output_img)\n",
        "print()\n",
        "output.append(max_output_img)\n",
        "titles.append('Max Pool output')\n",
        "\n",
        "#----------AVERAGE POOLING LAYER--------\n",
        "pool_layer = AvgPool2d(kernel_size=4, stride=4)\n",
        "op = pool_layer(input)\n",
        "avg_output_img = op.data.numpy()  # Tensor to back in numpy\n",
        "avg_output_img = avg_output_img.reshape((-1, avg_output_img.shape[-1]))\n",
        "print('Avg Pooling:\\n',avg_output_img)\n",
        "print()\n",
        "\n",
        "output.append(avg_output_img)\n",
        "titles.append('Avg Pool output')\n",
        "\n",
        "plot_images(output, titles, tick_params=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. Can you think of any other pooling other than max and avg?\n",
        "\n",
        "Answer)"
      ],
      "metadata": {
        "id": "cp9yq6LM8mBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides Max Pooling and Average Pooling, there are several other pooling operations:\n",
        "\n",
        "**1. Min Pooling**\n",
        "- Takes the minimum value in each pooling window\n",
        "- Less common, but useful for specific applications\n",
        "- Example: detecting dark features or finding minimum responses\n",
        "\n",
        "**2. Global Average Pooling (GAP)**\n",
        "- Pools each feature map into a single value by averaging all pixels\n",
        "- Replaces fully connected layers in modern architectures\n",
        "- Used in: GoogLeNet, ResNet, MobileNet\n",
        "- Benefits: Reduces parameters, prevents overfitting, enables variable input sizes\n",
        "\n",
        "**3. Global Max Pooling (GMP)**\n",
        "- Similar to GAP but takes the maximum value across entire feature map\n",
        "- Useful when you want the strongest activation signal\n",
        "- Common in attention mechanisms\n",
        "\n",
        "**4. Stochastic Pooling**\n",
        "- Randomly samples from pooling region based on activation probabilities\n",
        "- Probability proportional to activation magnitudes\n",
        "- Acts as regularization, reduces overfitting\n",
        "- Used during training; max pooling used during testing\n",
        "\n",
        "**5. L2 Pooling (L2-Norm Pooling)**\n",
        "- Takes the L2 norm (Euclidean norm) of values in pooling window\n",
        "- Formula: sqrt(sum of squares)\n",
        "- Preserves more information about magnitude\n",
        "- Less common in practice\n",
        "\n",
        "**6. Mixed Pooling**\n",
        "- Combines max and average pooling\n",
        "- Example: weighted combination like 0.5×max + 0.5×avg\n",
        "- Or randomly choose between max/avg during training\n",
        "\n",
        "**7. Fractional Max Pooling**\n",
        "- Uses non-integer stride/pooling sizes (e.g., stride=√2)\n",
        "- Creates overlapping pooling regions\n",
        "- Provides additional regularization\n",
        "\n",
        "**8. Spatial Pyramid Pooling (SPP)**\n",
        "- Pools at multiple scales simultaneously\n",
        "- Creates fixed-length representation from variable-sized inputs\n",
        "- Used in: SPPNet, Faster R-CNN\n",
        "- Enables CNNs to accept any input size\n",
        "\n",
        "**9. Gated Pooling**\n",
        "- Uses learnable gates to determine pooling weights\n",
        "- More adaptive than fixed max/avg pooling\n",
        "- Can learn task-specific pooling strategies\n",
        "\n",
        "**10. Detail-Preserving Pooling**\n",
        "- Designed to preserve fine details\n",
        "- Useful for segmentation tasks\n",
        "- Variants include detail-preserving max pooling\n",
        "\n",
        "**Most Common in Practice:**\n",
        "1. **Max Pooling** - Most popular, preserves strong features\n",
        "2. **Average Pooling** - Smoother downsampling\n",
        "3. **Global Average Pooling** - Replace FC layers, reduce parameters\n",
        "4. **Stochastic Pooling** - Regularization during training\n",
        "\n",
        "**Key Insight:**\n",
        "- Max pooling: Best for detecting presence of features\n",
        "- Average pooling: Best for smooth transitions and backgrounds\n",
        "- Global pooling: Best for reducing parameters and enabling variable input sizes\n",
        "- Other types: Specialized use cases\n",
        "\n",
        "**Modern Trend:**\n",
        "Many modern architectures use **strided convolutions** instead of pooling layers to reduce spatial dimensions while learning the downsampling operation.\n"
      ],
      "metadata": {
        "id": "H7-VR3weNeLh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQqIVNVYnA90"
      },
      "source": [
        "## 3. Fine-tuning and transfer learning\n",
        "\n",
        "Now , we will perform image classification using pretrained CNN models (transfer learning). We will understand two approaches, Fine-tuning and Feature extraction using ResNet architecture to train a model to perform traffic sign classification.\n",
        "\n",
        "To make your task easier, we provide you the starter code to perform the lab exercises. It is expected that you should try to understand what the code does and analyze the output. We will be using Pytorch framework for the implementation of this lab. The training hyperparameters that are used in the code may not be the best to minimize training time according to lab scope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7KBF9XrGaSy"
      },
      "source": [
        "### German Traffic Sign classification\n",
        "When a task involves training a CNN on a dataset of images, our first instinct would be to train the network from scratch. However, in practice, CNN has a huge number of parameters, often in the range of millions. Training a CNN on a small dataset greatly affects the network's ability to generalize, often resulting in overfitting.\n",
        "Therefore, in practice, one would fine-tune existing networks that are trained on a large dataset like the ImageNet (1.2M labeled images) by continue training it (i.e. running back-propagation) on the smaller dataset we have. Provided that our dataset is not drastically different in context to the original dataset (e.g. ImageNet), the pre-trained model will already have learned features that are relevant to our own classification problem.  Here, we will understand the Fine-tuning and Feature extraction approach to transfer learning. In the first one, we will take a pretrained ResNet model and replace the classifier to train it on our dataset. In the second approach, we will freeze the weights of the entire network except the classifier and train it on our data. We will thus, analyse the model performance in both cases. The German Traffic Sign Recognition Benchmark (GTSRB) dataset contains 43 classes of traffic signs, with varying light conditions and rich backgrounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ins6ETpApPJA"
      },
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHCZ__V1uK5R"
      },
      "source": [
        "# Device configuration (whether to run on GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4748CHsYoI2"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up9pTZ_fuROx"
      },
      "source": [
        "### Load German Traffic Sign dataset\n",
        "To get an idea of using our own datasets with Pytorch, this time, we will not use Pytorch's builtin datasets. The dataset we will use has more than 50K samples. To make the scenario more realistic, the number of samples in each class is limited to 200 only. And we have also reduced the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1V7dt70fz_AKRJlttyjnrtFpuJDLXr15x"
      ],
      "metadata": {
        "id": "dHm9cbhE2I5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d13S9bwuiQy"
      },
      "source": [
        "# Unzip\n",
        "!unzip -q german_traffic_signs_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HQXR2nRP3HJ"
      },
      "source": [
        "The dataset is stored in a folder structure where samples are separated in classwise folders. We can load the entire dataset using Pytorch's ['ImageFolder'](https://pytorch.org/vision/stable/datasets.html#ImageFolder) class. Then, we can see it like any built-in dataset. As the images are of varying shape, we will resize them to fixed dimensions (224,224) and normalize them in range [0,1]. We will here use data augmentation techniques like Gaussian blur and affine transformation to augment the data. This will increase variations in our data and help our model to generalize well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_183iPGunN4"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.GaussianBlur(3),\n",
        "            transforms.RandomAffine(0, translate=(0.3,0.3), shear=5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = ImageFolder('german_traffic_signs_dataset/Train', transform=transform)\n",
        "testset = ImageFolder('german_traffic_signs_dataset/Test', transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1yRmcSCu4Yu"
      },
      "source": [
        "#### Train, validation and test dataloaders\n",
        "We will split the trainset further to create train-validation split. We will only train on train data and evaluate the model on validation data at each step. The validation metrics helps us to understand whether model is overfitting the data or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GomEiTYu1rI"
      },
      "source": [
        "# Shuffle and split train set into 80% training and 20% validation set\n",
        "val_split = 0.2\n",
        "indices = np.arange(len(trainset))\n",
        "np.random.shuffle(indices)\n",
        "partition = int((1-val_split)*len(trainset))\n",
        "\n",
        "#SubsetRandomSampler will only sample examples from the given subset of data\n",
        "train_loader = DataLoader(trainset, shuffle=False, sampler=SubsetRandomSampler(indices[:partition]), batch_size=64, num_workers=2)\n",
        "val_loader = DataLoader(trainset, shuffle=False, sampler=SubsetRandomSampler(indices[partition:]), batch_size=64, num_workers=2)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': partition, 'val': len(train_loader.dataset) - partition}\n",
        "\n",
        "test_loader = DataLoader(testset, shuffle=False, batch_size=64, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxDtEgIZvI6r"
      },
      "source": [
        "# Print dataset information\n",
        "print('Number of training images: ', dataset_sizes['train'])\n",
        "print('Number of validation images: ', dataset_sizes['val'])\n",
        "print('Number of test images: ', len(test_loader.dataset))\n",
        "print('Number of classes: ', len(trainset.classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8xz7oSyvUCu"
      },
      "source": [
        "Helper functions for training/testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to show an image\n",
        "def plot_image(img):\n",
        "    img = img / 2 + 0.5                         # unnormalize the image\n",
        "    npimg = img.numpy()                         # torch to numpy\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # as torch image is (C, H, W)\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images from dataloader\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Plot images\n",
        "plot_image(torchvision.utils.make_grid(images[:20], nrow=5))\n"
      ],
      "metadata": {
        "id": "psftUClpcuCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKu5vTyaF-C"
      },
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    # best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    val_losses = []\n",
        "    val_acc = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                # Enable grads if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Running loss and correct predictions\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            # Save loss and acc values\n",
        "            if phase == 'train':\n",
        "              train_losses.append(epoch_loss)\n",
        "              train_acc.append(epoch_acc)\n",
        "            else:\n",
        "              val_losses.append(epoch_loss)\n",
        "              val_acc.append(epoch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc*100))\n",
        "\n",
        "            # Save the best validation accuracy\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc*100))\n",
        "\n",
        "    return train_losses, val_losses, train_acc, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZWTjtDwb8L_"
      },
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "      with torch.no_grad():\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model(images)\n",
        "        _, preds = torch.max(output, dim=1)\n",
        "        correct += (preds == labels).sum()\n",
        "\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print('Test Accuracy: {:.4f}'.format(test_acc*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QExzv8covZIR"
      },
      "source": [
        "### 1. Finetuning\n",
        "Here, we will load a pretrained model ResNet18 available in Pytorch and reset final fully connected layer. The model is trained on ImageNet dataset which is a large dataset containing 1000 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hG2XitCvX4B"
      },
      "source": [
        "# Load pretrained model\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Reset classifier to 43 output units (number of classes in our dataset)\n",
        "model.fc = nn.Linear(model.fc.in_features, 43)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC999ilMTx52"
      },
      "source": [
        "#### Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Sey-3y3UKJ"
      },
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7YjI7Y0T1f2"
      },
      "source": [
        "#### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNjCtOyBT419"
      },
      "source": [
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgpDdgB0wGAU"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaC-PUeNv8xJ"
      },
      "source": [
        "# Accuracy on test data before training\n",
        "test_model(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p5M87hTv9E7"
      },
      "source": [
        "history = train_model(model, criterion, optimizer, dataloaders, num_epochs=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS8lmAzgpPJE"
      },
      "source": [
        "Plot training plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iERYvlwpPJE"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(history[0])+1),history[0])\n",
        "ax.plot(np.arange(1,len(history[1])+1),history[1])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train Loss', 'Val Loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhVyadHIU9FY"
      },
      "source": [
        "#### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_4-zXsxVB3i"
      },
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Id93lgE1kdd"
      },
      "source": [
        "### 2. Feature Extraction\n",
        "Here, in the second approach, we will create a new instance of network and freeze entire network parameters except the final layer. We need to set ***requires_grad == False*** to freeze the parameters so that the gradients are not computed in backward()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfrJQuaVv9Hs"
      },
      "source": [
        "# Load pretrained model\n",
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all parameters\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of new classifier have requires_grad=True by default\n",
        "# so grads will be computed for classifier only\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 43)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgk1ahfE17Rp"
      },
      "source": [
        "# Evaluate model on test data before training\n",
        "print('Before training')\n",
        "test_model(model_conv, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNC6rLI32G65"
      },
      "source": [
        "####Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hLRYy2rv9OS"
      },
      "source": [
        "history = train_model(model_conv, criterion, optimizer_conv, dataloaders, num_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuSbIAfI2nnc"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(history[0])+1),history[0])\n",
        "ax.plot(np.arange(1,len(history[1])+1),history[1])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train Loss', 'Val Loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAvn3h1g2sR-"
      },
      "source": [
        "#### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ylntBgw2sR_"
      },
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model_conv, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSbdv4UPVfdp"
      },
      "source": [
        "As you can see, the test accuracy for feature extraction approach is not good compared to the first approach, although we are using pretrained models in both cases. Also, note that the training time was reduced to about half this time. This is obvious as we are not computing all the gradients this time. Experiment with the hyper-parameters like learning rate, epochs, and also optimizers till model convergence.Did you observe any improvement in the performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45jHKIHt6iM-"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mjppKyXWKaR"
      },
      "source": [
        "Q 1: Why do you think the network did not achieve good test accuracy in the feature extraction approach?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature extraction approach achieved lower test accuracy due to several reasons:\n",
        "\n",
        "**1. Domain Mismatch**\n",
        "- **ImageNet features** were learned from natural images (animals, objects, scenes)\n",
        "- **Traffic signs** have very different visual characteristics:\n",
        "  - Simple geometric shapes (circles, triangles, octagons)\n",
        "  - Bold, high-contrast colors (red, yellow, blue)\n",
        "  - Text and symbols rather than natural textures\n",
        "- Pre-trained features optimized for ImageNet may not be optimal for traffic signs\n",
        "\n",
        "**2. Frozen Feature Layers**\n",
        "- All convolutional layers remain frozen (not trainable)\n",
        "- Only the final classifier is trained\n",
        "- Cannot adapt low-level and mid-level features to the new task\n",
        "- Stuck with features that may not be relevant for traffic signs\n",
        "\n",
        "**3. Limited Adaptability**\n",
        "- Traffic signs require recognizing:\n",
        "  - Specific shapes (stop sign octagon, yield triangle)\n",
        "  - Color patterns (red circle with white bar)\n",
        "  - Small textual/symbolic details\n",
        "- ImageNet features focus on:\n",
        "  - Natural textures (fur, leaves, water)\n",
        "  - Complex objects (animals, vehicles)\n",
        "  - Scene understanding\n",
        "- The mismatch reduces classification performance\n",
        "\n",
        "**4. Loss of Discriminative Power**\n",
        "- Early layers learn edges, textures, basic patterns\n",
        "- Middle layers learn parts, shapes, combinations\n",
        "- Later layers learn high-level semantics\n",
        "- For traffic signs, we might need different mid-level features\n",
        "- Since we can't train these, we lose discriminative power\n",
        "\n",
        "**5. Small Dataset**\n",
        "- Traffic sign datasets are typically smaller than ImageNet\n",
        "- With frozen features, we only train a small classifier\n",
        "- Not enough trainable parameters to fully compensate for feature mismatch\n",
        "\n",
        "**Solution**: Fine-tuning would allow the network to adapt features, especially in later layers, leading to better performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "v1iQ-B4ML0nC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-FfAbYBW_qA"
      },
      "source": [
        "Q 2: Can you think of a scenario where the feature extraction approach would be preferred compared to fine tuning approach?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes! Feature extraction is preferred in several scenarios:\n",
        "\n",
        "**1. Very Small Dataset**\n",
        "- **When**: You have < 100-1000 samples per class\n",
        "- **Why**: Fine-tuning risks overfitting with so little data\n",
        "- **Example**: Medical diagnosis with only 50 samples\n",
        "- Feature extraction uses robust pre-trained features without overfitting\n",
        "\n",
        "**2. Limited Computational Resources**\n",
        "- **When**: Training on CPU or limited GPU memory\n",
        "- **Why**: Feature extraction is 2-5× faster (only trains final layer)\n",
        "- **Example**: Embedded systems, mobile applications, edge devices\n",
        "- Lower memory requirements (don't need to store gradients for all layers)\n",
        "\n",
        "**3. Very Similar Domain to Pre-trained Model**\n",
        "- **When**: Your task is very similar to ImageNet\n",
        "- **Why**: Pre-trained features are already optimal\n",
        "- **Example**: Fine-grained classification (dog breeds, bird species, car models)\n",
        "- ImageNet already learned these features well\n",
        "\n",
        "**4. Real-Time or Production Constraints**\n",
        "- **When**: Need fast training/deployment cycles\n",
        "- **Why**: Quick to train, easier to update\n",
        "- **Example**: A/B testing different classifiers, rapid prototyping\n",
        "- Can train new classifier in minutes rather than hours\n",
        "\n",
        "**5. Interpretability and Stability**\n",
        "- **When**: Need consistent, interpretable features\n",
        "- **Why**: Fixed features are more stable across retraining\n",
        "- **Example**: Medical/legal applications requiring reproducibility\n",
        "- Features don't change when you retrain on new data\n",
        "\n",
        "**6. Transfer to Multiple Related Tasks**\n",
        "- **When**: Same features work for multiple downstream tasks\n",
        "- **Why**: Train one feature extractor, use for many classifiers\n",
        "- **Example**:\n",
        "  - Same ResNet features for: vehicle detection, vehicle classification, vehicle color classification\n",
        "  - Saves storage (one model) and training time\n",
        "\n",
        "**7. Preventing Catastrophic Forgetting**\n",
        "- **When**: Need to preserve original model capabilities\n",
        "- **Why**: Fine-tuning might destroy original ImageNet performance\n",
        "- **Example**: Multi-task scenarios where you need both ImageNet and custom task\n",
        "\n",
        "**8. Quick Baseline Establishment**\n",
        "- **When**: Starting a new project\n",
        "- **Why**: Fast way to get initial results\n",
        "- **Example**: Establish baseline performance before investing in full fine-tuning\n",
        "\n",
        "**Best Practice**:\n",
        "1. Start with feature extraction (quick baseline)\n",
        "2. If performance is insufficient, try fine-tuning\n",
        "3. If dataset is small, use data augmentation + feature extraction"
      ],
      "metadata": {
        "id": "q3jqJyLjL3vC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ2TU3_uawLc"
      },
      "source": [
        "Q 3: Replace the ResNet18 architecture with some other pretrained model in pytorch and try to find the optimal parameters. Report the architecture and the final model performance.\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recommended Alternative Architectures:**\n",
        "\n",
        "```python\n",
        "import torchvision.models as models\n",
        "\n",
        "# Option 1: VGG16 - Simpler, deeper\n",
        "model = models.vgg16(pretrained=True)\n",
        "# Modify classifier:\n",
        "model.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "# Option 2: EfficientNet-B0 - Most efficient\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "# Modify classifier:\n",
        "model.classifier[1] = nn.Linear(1280, num_classes)\n",
        "\n",
        "# Option 3: MobileNetV2 - Fastest inference\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "# Modify classifier:\n",
        "model.classifier[1] = nn.Linear(1280, num_classes)\n",
        "\n",
        "# Option 4: DenseNet121 - Best gradient flow\n",
        "model = models.densenet121(pretrained=True)\n",
        "# Modify classifier:\n",
        "model.classifier = nn.Linear(1024, num_classes)\n",
        "```\n",
        "\n",
        "**Recommended Architecture**: **EfficientNet-B0**\n",
        "\n",
        "**Why EfficientNet-B0:**\n",
        "1. Superior accuracy-to-parameters ratio\n",
        "2. Compound scaling (depth, width, resolution)\n",
        "3. Better performance than ResNet18 with similar complexity\n",
        "4. Modern architecture (2019) with best practices\n",
        "\n",
        "**Optimal Hyperparameters for Traffic Signs:**\n",
        "\n",
        "```python\n",
        "# Model\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, 43)  # 43 traffic sign classes\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                   factor=0.5, patience=3)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "# Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "```\n",
        "\n",
        "**Expected Performance:**\n",
        "- **ResNet18**: ~92-95% test accuracy\n",
        "- **EfficientNet-B0**: ~95-98% test accuracy\n",
        "- **VGG16**: ~90-93% test accuracy\n",
        "- **MobileNetV2**: ~91-94% test accuracy\n",
        "\n",
        "**Training Strategy:**\n",
        "1. Fine-tune with all layers (initially with small LR)\n",
        "2. Or: Freeze early layers, train last few blocks\n",
        "3. Use learning rate scheduling\n",
        "4. Apply heavy data augmentation"
      ],
      "metadata": {
        "id": "lY_PaCYbMEXn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOHxNtgt3qM2"
      },
      "source": [
        "Q 4: Which other data augmentations can we used to augment the data?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Data Augmentations for Traffic Signs:**\n",
        "\n",
        "**1. Geometric Transformations**\n",
        "```python\n",
        "transforms.RandomRotation(degrees=15)           # Slight rotations\n",
        "transforms.RandomAffine(degrees=0,\n",
        "                        translate=(0.1, 0.1),    # Shift position\n",
        "                        scale=(0.9, 1.1))        # Zoom in/out\n",
        "transforms.RandomPerspective(distortion_scale=0.2)  # Perspective changes\n",
        "transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0))\n",
        "```\n",
        "\n",
        "**2. Color/Intensity Augmentations**\n",
        "```python\n",
        "transforms.ColorJitter(brightness=0.3,          # Lighting conditions\n",
        "                       contrast=0.3,            # Contrast variation\n",
        "                       saturation=0.3,          # Color saturation\n",
        "                       hue=0.1)                 # Slight hue shift\n",
        "transforms.RandomGrayscale(p=0.1)               # Occasional grayscale\n",
        "transforms.RandomAdjustSharpness(sharpness_factor=2)\n",
        "transforms.RandomAutocontrast()\n",
        "```\n",
        "\n",
        "**3. Noise and Blur**\n",
        "```python\n",
        "# Add Gaussian noise\n",
        "class AddGaussianNoise:\n",
        "    def __init__(self, mean=0., std=0.1):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    \n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "# Gaussian blur (simulates camera shake, motion)\n",
        "transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "```\n",
        "\n",
        "**4. Weather/Environmental Conditions**\n",
        "```python\n",
        "# Rain effect\n",
        "class AddRainEffect:\n",
        "    def __call__(self, img):\n",
        "        # Add random white streaks\n",
        "        ...\n",
        "\n",
        "# Fog/haze effect\n",
        "class AddFog:\n",
        "    def __call__(self, img):\n",
        "        # Reduce contrast, add whiteness\n",
        "        ...\n",
        "\n",
        "# Shadows\n",
        "class AddShadow:\n",
        "    def __call__(self, img):\n",
        "        # Darken random regions\n",
        "        ...\n",
        "```\n",
        "\n",
        "**5. Occlusion/Cutout**\n",
        "```python\n",
        "# Random erasing (simulates partial occlusion)\n",
        "transforms.RandomErasing(p=0.5, scale=(0.02, 0.15))\n",
        "\n",
        "# Cutout (remove random patches)\n",
        "class Cutout:\n",
        "    def __init__(self, n_holes=1, length=16):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "```\n",
        "\n",
        "**6. MixUp / CutMix (Advanced)**\n",
        "```python\n",
        "# MixUp: blend two images\n",
        "def mixup(img1, img2, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    return lam * img1 + (1 - lam) * img2\n",
        "\n",
        "# CutMix: paste patch from one image onto another\n",
        "```\n",
        "\n",
        "**7. AutoAugment / RandAugment**\n",
        "```python\n",
        "from torchvision.transforms import autoaugment\n",
        "\n",
        "transforms.AutoAugment(autoaugment.AutoAugmentPolicy.IMAGENET)\n",
        "transforms.RandAugment(num_ops=2, magnitude=9)\n",
        "```\n",
        "\n",
        "**Recommended Augmentation Pipeline for Traffic Signs:**\n",
        "\n",
        "```python\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10))\n",
        "])\n",
        "```\n",
        "\n",
        "**Best Practices:**\n",
        "1. Don't over-augment - keep signs recognizable\n",
        "2. Test augmentations visually before training\n",
        "3. Start conservative, increase if overfitting\n",
        "4. Different augmentations for different domains\n",
        "5. Use domain-specific augmentations (weather for outdoor scenes)"
      ],
      "metadata": {
        "id": "l0lblokpMkx5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsalBk66WZIn"
      },
      "source": [
        "## References and Additional Resources:\n",
        "\n",
        "*   [Transfer Learning Pytorch tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#id1)\n",
        "*   [Transfer Learning with Convolutional Neural Networks in PyTorch](https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce)\n",
        "*    [Torchvision models](https://pytorch.org/vision/stable/models.html)\n",
        "*    [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Rm8FB-Qn4K"
      },
      "source": [
        "## References and Additional Resources:\n",
        "\n",
        "*   [Training a classifier tutorial - Pytorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-an-image-classifier)\n",
        "*   [Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)\n",
        "*   [ConvNetJS CIFAR10 Demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some more experiments"
      ],
      "metadata": {
        "id": "yVGV_2qaKmpF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f608636"
      },
      "source": [
        "\n",
        "Now let us update the `apply_filter` function to accept `padding` and `stride` parameters and implement the logic for both.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dead65ca"
      },
      "source": [
        "def apply_filter(img, filter, padding=0, stride=1):\n",
        "  # Handle padding first\n",
        "  if padding > 0:\n",
        "    # Pad the image with constant values (zeros)\n",
        "    padded_img = np.pad(img, padding, mode='constant', constant_values=0)\n",
        "  else:\n",
        "    padded_img = img\n",
        "\n",
        "  input_height, input_width = padded_img.shape\n",
        "  filter_height, filter_width = filter.shape\n",
        "\n",
        "  # Calculate the output dimensions based on input size, filter size, padding, and stride\n",
        "  # Formula: (input_dim - filter_dim) / stride + 1\n",
        "  output_height = (input_height - filter_height) // stride + 1\n",
        "  output_width = (input_width - filter_width) // stride + 1\n",
        "\n",
        "  # Initialize the output array with the calculated dimensions\n",
        "  output = np.zeros((output_height, output_width))\n",
        "\n",
        "  # Move the filter over the padded image\n",
        "  # Iterate for output rows\n",
        "  for i_out in range(output_height):\n",
        "    i_in = i_out * stride # Determine the starting row in the input image for the current output row\n",
        "    # Iterate for output columns\n",
        "    for j_out in range(output_width):\n",
        "      j_in = j_out * stride # Determine the starting column in the input image for the current output column\n",
        "\n",
        "      # Extract the current patch from the padded image\n",
        "      patch = padded_img[i_in : i_in + filter_height, j_in : j_in + filter_width]\n",
        "\n",
        "      # Perform element-wise multiplication and sum, then assign to the output array\n",
        "      output[i_out, j_out] = np.sum(np.multiply(patch, filter))\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89749299"
      },
      "source": [
        "### Padding Examples\n",
        "\n",
        "Let's see the effect of padding on the output image size and content using our `apply_filter` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92d77b27"
      },
      "source": [
        "# Ensure all necessary imports are available locally\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Redefine plot_images function to ensure it's in scope\n",
        "def plot_images(images, titles, tick_params=True):\n",
        "  n = len(images)\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(1,n,i+1)\n",
        "    if len(images[i].shape) == 2:\n",
        "      ax.imshow(images[i], cmap='gray',\n",
        "                extent=(0,images[i].shape[1], images[i].shape[0], 0))\n",
        "    else:\n",
        "      ax.imshow(images[i])\n",
        "    ax.set_title(titles[i])\n",
        "    if not tick_params:\n",
        "      plt.tick_params(axis='both', labelbottom=False, bottom=False,\n",
        "                labelleft=False, left=False)\n",
        "  plt.show()\n",
        "\n",
        "# Ensure lotus.jpg is available by re-downloading it if needed\n",
        "!curl -L -o 'lotus.jpg' 'https://drive.google.com/uc?export=download&id=1gQSQlrUws22KLRUacXwvN1G8FtIyhfGt'\n",
        "\n",
        "# Reload lotus.jpg for consistent demonstration\n",
        "image_lotus = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if image_lotus is None or image_lotus.size == 0:\n",
        "    print(\"Warning: Could not load 'lotus.jpg' or it's empty. Please ensure the file exists and is valid.\")\n",
        "    # Create a dummy image to avoid further errors for demonstration purposes\n",
        "    image_lotus = np.zeros((100, 100), dtype=np.uint8) # Create a 100x100 black image\n",
        "    print(\"Proceeding with a dummy image.\")\n",
        "\n",
        "# Vertical edge filter\n",
        "filter_vertical = np.array([[1,0,-1],\n",
        "                            [1,0,-1],\n",
        "                            [1,0,-1]])\n",
        "\n",
        "# Apply filter with padding=1\n",
        "padded_output = apply_filter(image_lotus, filter_vertical, padding=1)\n",
        "\n",
        "print(f\"Original image shape: {image_lotus.shape}\")\n",
        "print(f\"Output image shape with padding=1: {padded_output.shape}\")\n",
        "\n",
        "images_pad = [image_lotus, filter_vertical, padded_output]\n",
        "titles_pad = ['Original Image', 'Vertical Filter', 'Output with Padding=1']\n",
        "plot_images(images_pad, titles_pad)\n",
        "\n",
        "# Apply filter with padding=2\n",
        "padded_output_2 = apply_filter(image_lotus, filter_vertical, padding=2)\n",
        "print(f\"Output image shape with padding=2: {padded_output_2.shape}\")\n",
        "\n",
        "images_pad_2 = [image_lotus, filter_vertical, padded_output_2]\n",
        "titles_pad_2 = ['Original Image', 'Vertical Filter', 'Output with Padding=2']\n",
        "plot_images(images_pad_2, titles_pad_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701d89fe"
      },
      "source": [
        "### Stride Examples\n",
        "\n",
        "Now, let's observe how stride affects the output feature map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbf38a2c"
      },
      "source": [
        "# Reload lotus.jpg for consistent demonstration\n",
        "image_lotus = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "# Check if the image was loaded successfully (similar to padding example)\n",
        "if image_lotus is None or image_lotus.size == 0:\n",
        "    print(\"Warning: Could not load 'lotus.jpg' or it's empty. Please ensure the file exists and is valid.\")\n",
        "    image_lotus = np.zeros((100, 100), dtype=np.uint8)\n",
        "    print(\"Proceeding with a dummy image.\")\n",
        "\n",
        "# Vertical edge filter\n",
        "filter_vertical = np.array([[1,0,-1],\n",
        "                            [1,0,-1],\n",
        "                            [1,0,-1]])\n",
        "\n",
        "# Apply filter with stride=1 (default behavior, for comparison)\n",
        "stride_output_1 = apply_filter(image_lotus, filter_vertical, stride=1)\n",
        "\n",
        "print(f\"Original image shape: {image_lotus.shape}\")\n",
        "print(f\"Output image shape with stride=1: {stride_output_1.shape}\")\n",
        "\n",
        "images_stride_1 = [image_lotus, filter_vertical, stride_output_1]\n",
        "titles_stride_1 = ['Original Image', 'Vertical Filter', 'Output with Stride=1']\n",
        "plot_images(images_stride_1, titles_stride_1)\n",
        "\n",
        "\n",
        "# Apply filter with stride=2\n",
        "stride_output_2 = apply_filter(image_lotus, filter_vertical, stride=2)\n",
        "print(f\"Output image shape with stride=2: {stride_output_2.shape}\")\n",
        "\n",
        "images_stride_2 = [image_lotus, filter_vertical, stride_output_2]\n",
        "titles_stride_2 = ['Original Image', 'Vertical Filter', 'Output with Stride=2']\n",
        "plot_images(images_stride_2, titles_stride_2)\n",
        "\n",
        "# Apply filter with stride=3\n",
        "stride_output_3 = apply_filter(image_lotus, filter_vertical, stride=3)\n",
        "print(f\"Output image shape with stride=3: {stride_output_3.shape}\")\n",
        "\n",
        "images_stride_3 = [image_lotus, filter_vertical, stride_output_3]\n",
        "titles_stride_3 = ['Original Image', 'Vertical Filter', 'Output with Stride=3']\n",
        "plot_images(images_stride_3, titles_stride_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e56fc65"
      },
      "source": [
        "### Question\n",
        "\n",
        "What are the primary benefits and drawbacks of using padding and stride in convolutional operations? When might you choose to use one over the other, or both?\n",
        "\n",
        "---\n",
        "\n",
        "### Answer\n",
        "\n",
        "Padding and stride are important hyperparameters in convolutional neural networks, each offering distinct advantages and trade-offs.\n",
        "\n",
        "---\n",
        "\n",
        "### Padding\n",
        "\n",
        "**Benefits**\n",
        "- Preserves spatial information at the image boundaries  \n",
        "- Helps maintain or control output size (e.g., *same* padding)  \n",
        "- Enables deeper networks without rapid shrinking of feature maps  \n",
        "- Improves learning of edge and corner features  \n",
        "\n",
        "**Drawbacks**\n",
        "- Increases computational cost due to larger effective input  \n",
        "- Introduces artificial values (such as zeros), which may slightly affect boundary activations  \n",
        "\n",
        "**When to use padding**\n",
        "- When preserving spatial dimensions is important  \n",
        "- In early convolutional layers to capture edge information  \n",
        "- In deep architectures to avoid excessive spatial reduction  \n",
        "\n",
        "---\n",
        "\n",
        "### Stride\n",
        "\n",
        "**Benefits**\n",
        "- Efficiently reduces spatial dimensions (acts as downsampling)  \n",
        "- Lowers computation and memory requirements  \n",
        "- Expands receptive field more quickly  \n",
        "\n",
        "**Drawbacks**\n",
        "- May discard fine-grained spatial details  \n",
        "- Large stride values can cause loss of important information  \n",
        "\n",
        "**When to use stride**\n",
        "- When downsampling is desired instead of pooling  \n",
        "- In deeper layers where precise spatial detail is less critical  \n",
        "- When computational efficiency is a priority  \n",
        "\n",
        "---\n",
        "\n",
        "### Using Padding and Stride Together\n",
        "\n",
        "Padding and stride are commonly used together to balance information retention and efficiency:\n",
        "- Padding helps preserve boundary information  \n",
        "- Stride controls resolution and computational cost  \n",
        "\n",
        "**Typical design choice**\n",
        "- Early layers: small stride with padding (retain details)  \n",
        "- Later layers: larger stride with padding (reduce resolution while maintaining context)\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "- Use **padding** to preserve spatial information  \n",
        "- Use **stride** to control output size and computational efficiency  \n",
        "- Use **both** to build stable and efficient convolutional networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b7a1c8"
      },
      "source": [
        "## Some more questions\n",
        "\n",
        "1. Does increasing stride increase output image size?\n",
        "\n",
        "Answer\n",
        " No, increasing the stride value decreases the output image size. Stride determines how many pixels the filter shifts over the input image at each step. A larger stride means the filter skips more pixels, resulting in fewer computations and a smaller output feature map.\n",
        "\n",
        "2. Does increasing padding increase output image size?\n",
        "\n",
        "Answer\n",
        " Yes, increasing the padding value increases the output image size (or helps maintain it). Padding involves adding extra rows and columns of zeros (or other values) around the border of the input image. This allows the filter to process pixels closer to the edges and can help preserve the spatial dimensions of the input image in the output.\n"
      ]
    }
  ]
}